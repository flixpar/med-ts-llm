DEBUG = false

task = "pretraining" # options: forecasting, anomaly_detection, semantic_segmentation, segmentation, pretraining
model = "timellm"    # options: timellm, gpt4ts, dlinear, patchtst, fedformer, timesnet

history_len = 128
pred_len = 128


[data]
dataset = "all"
mode = "multivariate"
cols = "all"
normalize = true
step = 64


[training]
epochs = 10
batch_size = 16
optimizer = "ranger"
learning_rate = 1e-4
dropout = 0.1
loss = "mse"

eval_metric = "mse"
eval_metric_direction = "min"


[tasks.pretraining]
tasks = ["forecasting"]
datasets = "all"
downsample_pct = 0.2
n_features = 3


[models.timellm]
d_model = 32
d_ff = 64
n_heads = 8
num_tokens = 1000
covariate_mode = "add"                # options: univariate, concat, add, interleave, independent, independent-weighted
embedding_downsample_mode = "linear"  # options: linear, truncate, average

[models.timellm.patching]
patch_len = 16
stride = 8

[models.timellm.llm]
enabled = true
llm = "state-spaces/mamba-1.4b-hf"
load_in_4bit = false
load_in_8bit = false
llm_layers = 36


[setup]
seed = 0
device = "auto"
dtype = "bf16"
num_workers = "auto"
logger = "wandb"
